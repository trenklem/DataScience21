{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in Python with Scikit-Learn\n",
    "<img src=\"IMG/sk-logo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print('running in Colab:',IN_COLAB)\n",
    "path='..'\n",
    "if IN_COLAB:\n",
    "  #in colab, we need to clone the data from the repo\n",
    "  !git clone https://github.com/keuperj/DataScienceSS20.git\n",
    "  path='DataScienceSS20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-Learn Overview\n",
    "<img src=\"IMG/sk-logo.png\" width=200>\n",
    "\n",
    "* dominant Machine Learning Library for Python\n",
    "* very wide user basis\n",
    "* very good documentation\n",
    "* state of the art  implementation \n",
    "* unified API \n",
    "* full integration in ***NumPy / Pandas*** work flows\n",
    "* *everything but* **Deep Learning**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-Learn Resources\n",
    "<img src=\"IMG/sk-logo.png\" width=200>\n",
    "\n",
    "* Website: https://scikit-learn.org/stable/index.html\n",
    "* API Reference: https://scikit-learn.org/stable/modules/classes.html\n",
    "* Tutorial: https://scikit-learn.org/stable/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-Learn Structure\n",
    "<img src=\"IMG/sk-overview.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-Learn Structure\n",
    "<img src=\"IMG/sk-logo.png\" width=200>\n",
    "\n",
    "***SkLearn*** provides a wide range of ML Algorithms plus methods for:\n",
    "* loading / accessing data\n",
    "* data pre-processing\n",
    "* data selection\n",
    "* model evaluation \n",
    "* model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Access\n",
    "### Build in Data Sets\n",
    "***SkLearn*** provides many datasets that are commonly used in Machine Learning teaching and tutorials.\n",
    "* see full list here: https://scikit-learn.org/stable/datasets/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X=load_iris()['data'] #vectors of data\n",
    "Y=load_iris()['target'] #label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X[:20,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unified API\n",
    "One key feature of ***SkLearn*** is it's unified API, that allows a very simple exchange ML methods:   \n",
    "\n",
    "1. create **model instance** for ML Algorithm *A* \n",
    "```\n",
    "model = A( SOME_METHOD_SPECIFIC_PARAMETERS)\n",
    "```\n",
    "2. **train** model with data X (and labels Y if we use ***supervised ML***) \n",
    "```\n",
    "model.fit(X) or model.fit(X,Y)\n",
    "```\n",
    "3. **inference** of data X_test on our model \n",
    "```\n",
    "pred = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Simple Classification Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "#generate random date for classification\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#randomly split into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plot problem\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X[:,0],X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def plot_surface(clf, X, y, \n",
    "                 xlim=(-10, 10), ylim=(-10, 10), n_steps=250, \n",
    "                 subplot=None, show=True):\n",
    "    if subplot is None:\n",
    "        fig = plt.figure()\n",
    "    else:\n",
    "        plt.subplot(*subplot)\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], n_steps), \n",
    "                         np.linspace(ylim[0], ylim[1], n_steps))\n",
    "    \n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        \n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, alpha=0.8, cmap=plt.cm.RdBu_r)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.xlim(*xlim)\n",
    "    plt.ylim(*ylim)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#train first algorithm: Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "#plotting model confidence \n",
    "plot_surface(model,X_train,y_train, (-2,2), (0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#now the same problem with a different algorithem: Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "#plotting model confidence \n",
    "plot_surface(model,X_train,y_train, (-2,2), (0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot inference\n",
    "plot_surface(model,X_test,pred, (-2,2), (0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#see if model is correct \n",
    "pred==y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving and Loading Models\n",
    "Models are stored via ***pickle***, the ***Python*** serialization library https://docs.python.org/3/library/pickle.html. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(model, open( \"my_model.p\", \"wb\" ) ) #seave model to fiel\n",
    "model2 = pickle.load(open( \"my_model.p\", \"rb\" ) )#load model from firl\n",
    "model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-Processing\n",
    "***SkLearn*** provides a wide range of pre-processing methods on ***NumPy*** arrays and other input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example scaling data\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "X_scaled[:20,:]                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scaling\n",
    "One problem with scaling - as with all other pre-processing methods - is, that we need to find the \"right\" processing steps based on the **train data** and the also apply it to the **test data**. ***SkLearn*** provides ***Scaler*** models to do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler.mean_ #get model mean                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_ #get scales                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler.transform(X_train)                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scaler\n",
    "There are many different ***Scaler*** available. See [Examples here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization\n",
    "***Normalization*** is the process of scaling **individual samples** to have ***unit norm***. Works just like scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer(norm='l2').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Encoding categorical features\n",
    "Often features are not given as continuous values but categorical. For example a person could have features\n",
    "```[\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. \n",
    "```\n",
    "<br><br>\n",
    "Such features can be efficiently coded as integers, for instance ``[\"male\", \"from US\", \"uses Internet Explorer\"]`` could be expressed as [0, 1, 3] while ``[\"female\", \"from Asia\", \"uses Chrome\"]`` would be [1, 2, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#sklearn can do this out-of the box\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)  \n",
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc.transform([['male', 'from Europe', 'uses Firefox']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### One-Hot Encoding\n",
    "Another possibility to convert categorical features to features is to use a ***one-hot*** or dummy encoding. This transforms each categorical feature with **$n$ categories** possible values into **$n$ categories binary features**, with one of them 1, and all others 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)  \n",
    "\n",
    "enc.transform([['female', 'from US', 'uses Safari'],['male', 'from Europe', 'uses Safari']]).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#discretize data by dimension\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2], encode='ordinal').fit(X_train)\n",
    "est.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom Transformers\n",
    "***SkLEarn*** also has an easy interface for custom transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def myTrans(x):\n",
    "    return np.log1p(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FunctionTransformer(myTrans)\n",
    "transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines\n",
    "***Pipeline*** can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "* Convenience and encapsulation\n",
    "    \n",
    "* Joint parameter selection\n",
    "   \n",
    "* Safety\n",
    "   \n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "normalizer = preprocessing.Normalizer(norm='l2')\n",
    "model = RandomForestClassifier()\n",
    "myPipeline = make_pipeline(normalizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now train it\n",
    "myPipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myPipeline.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now... hands on!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "enable_chalkboard": true,
   "footer": "Janis Keuper - SS21",
   "header": "Data Science: Block 4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
